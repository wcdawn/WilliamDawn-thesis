\chapter{Finite Element Neutron Diffusion}
\label{ch:neutronDiffusion}

\section{Introduction}
  % change this to begin with multigroup
  For typical SFR applications, diffusion theory
  approximates the neutron distribution within the reactor well. The multigroup
  neutron diffusion equation is a second order partial differential equation in
  space. The diffusion approximation is valid for fast reactors because neutron
  mean-free-paths within the reactor are large.

  Spatial discretization will be done with the Finite 
  Element Method (FEM). This spatial discretization method is selected for 
  several reasons. It allows for easily increasing the spatial convergence order
  of the method by increasing the order of the elements without refining the
  mesh. For example, using the same mesh, quadratic elements instead of linear
  elements could be used to refine the solution. Coordinates of nodes and 
  elements can be easily updated to reflect physical phenomena, such as thermal 
  expansion (\chref{ch:thermalExpansion}). Additionally, material properties are 
  calculated on an element basis allowing for fine detail updates to the 
  material properties during the calculation in future applications.

\section{Multigroup Neutron Diffusion Equation}
  In the multigroup neutron diffusion equation, an energy structure is 
  described as $\{E_g\}$ for $g = 0,1,2,\ldots,G$ and by in order of decreasing 
  energy by convention.
  \[ 0 < E_G < E_{G-1} < \ldots < E_2 < E_1 < E_0 \]
  Then, multigroup constants can be calculated based on the energy group 
  structure, known cross-sections, and a representative flux spectrum.
  Generation of multigroup constants is performed using \mcc and is described in
  \sref{sec:cross_section_treatment}.
  In conventional notation, the multigroup neutron diffusion equation can be
  written as 
  \begin{equation}
    \label{eq:multigroup_diffusion}
    - \grad \cdot ( D_g(\vr) \grad \phi_g(\vr)) + \Sigma_{t,g}(\vr) \phi_g(\vr)= 
      \frac{\chi_g(\vr)}{\keff} \sum_{g'=1}^{G} \nu\Sigma_{f,g'}(\vr) 
      \phi_{g'}(\vr) + \sum_{g'=1}^{G} \Sigma_{s,g' \rightarrow g}(\vr) 
      \phi_{g'}(\vr)
  \end{equation}
  where $D_g$ is the diffusion constant, $\phi_g$ is the neutron flux, 
  $\Sigma_{t,g}$ is the total cross-section, $\chi_g$ is the 
  fission neutron spectrum, $\keff$ is the effective neutron multiplication 
  factor, $\nu \Sigma_{f,g}$ is the number of fission neutrons generated per
  unit neutron flux,
  and $\Sigma_{s,g' \rightarrow g}$ is the scattering cross 
  section for neutrons scattering from energy group $E_g'$ to energy group
  $E_g$. Spatial dicretization will be based on the Finite Element Method (FEM)
  and will be discussed in \sref{sec:formulation}.
  
  The total neutron cross-section includes the contribution due to 
  self-scattering. That is, due to $\Sigma_{s,g\rightarrow g}$. This can be 
  subtracted from both sides of \eref{eq:multigroup_diffusion} for simplicity 
  and numeric efficiency.
  \begin{equation} \label{eq:multigroup_removal}
    - \grad \cdot( D_g(\vr) \grad \phi_g(\vr)) + \Sigma_{r,g}(\vr) \phi_g(\vr) = 
      \frac{\chi_g(\vr)}{\keff} \sum_{g'=1}^{G} \nu\Sigma_{f,g'}(\vr) 
      \phi_{g'}(\vr) + \sum_{g'=1, g' \ne g}^{G} 
      \Sigma_{s,g' \rightarrow g}(\vr) \phi_{g'}(\vr)
  \end{equation}
  Where $\Sigma_{r,g}$ is the removal cross-section defined as 
  $\Sigma_{r,g}(\vr) = \Sigma_{t,g}(\vr) - \Sigma_{s,g\rightarrow g}(\vr)$. For
  simplicity, the neutron sources in \eref{eq:multigroup_removal} can be 
  combined into a single term.
  \begin{equation} \label{eq:multigroup_source}
    - \grad \cdot( D_g(\vr) \grad \phi_g(\vr)) + \Sigma_{r,g}(\vr) \phi_g(\vr) = 
      q_g(\vr)
  \end{equation}
  where $q_g(\vr)$ is the combined neutron source at position $\vr$ for energy
  group $g$ and is
  expressed as
  \begin{equation}
    \label{eq:q}
    q_g(\vr) = q_{fiss,g}(\vr) + q_{up,g}(\vr) + q_{down,g}(\vr) \\
  \end{equation}
  with contributing terms
  \begin{align}
    \label{eq:qfiss}
    q_{fiss,g}(\vr) &= \frac{\chi_g(\vr)}{\keff} \sum_{g'=1}^{G} 
      \nu \Sigma_{f,g'}(\vr) \phi_{g'}(\vr) \\
    \label{eq:qup}
    q_{up,g}(\vr) &= \sum_{g'=g+1}^{G} \Sigma_{s,g' \rightarrow g}(\vr)
      \phi_{g'}(\vr) \\
    \label{eq:qdown}
    q_{down,g}(\vr) &= \sum_{g'=1}^{g-1} \Sigma_{s,g' \rightarrow g}(\vr)
      \phi_{g'}(\vr)
  \end{align}
  Where the difference between $q_{up}$ and $q_{down}$ are the limits of the 
  summation. This form allows for operator splitting of the neutron source term.
  In an iterative scheme, it will be necessary for fission and up-scatter 
  sources to use a different flux iterate than down-scatter so this division
  will prove useful.

  The combined source form is useful for solving the
  multigroup neutron diffusion problem for an arbitrary number of groups.
  \eref{eq:multigroup_source}
  is solved for each energy group and interaction between groups is
  described in the source term, $q_g(\vr)$. In other literature, the multigroup
  equation may be solved for all groups simultaneously by treating interaction 
  between groups explicitly. By solving each group independently (as done here)
  the method remains general. Additionally, for many-group energy structures, as
  common to fast reactor applications, solving each group independently is 
  typically more computationally efficient as linear systems have favorable 
  conditioning and remain smaller. Fast reactors also are dominated by
  down-scatter as opposed to thermal reactors which experience significant
  up-scatter.

\section{Formulation of Finite Element Equations}
  \label{sec:formulation}
  \subsection{Derivation}
    \label{sec:formulation:derivation}
    The only remaining continuous variable in the problem to be discretized is 
    the spatial variable $\vr$. This will be discretized according to the Finite 
    Element  Method (FEM). The problem is solved in a finite domain 
    $\vr \in \Omega$ where $\partial \Omega$ represents the boundary of the 
    domain where some boundary condition is specified. Boundary condition 
    options provided include
    \begin{enumerate}
      \item Mirror. $\grad \phi_g(\vr) \cdot \nhat = 0$ for 
        $\vr \in \partial \Omega$.
      \item Albedo. $D_g(\vr) \grad \phi_g(\vr) \cdot \nhat + 
        \albedo \phi_g(\vr)=0$ for $\vr \in \partial \Omega$ 
        where $\albedo$ is a real constant specified
        by the user. For vacuum conditions, $\albedo = \half$.
      \item Zero Flux. $\phi_g(\vr) = 0$ for $\vr \in \partial \Omega$.
    \end{enumerate}
    where $\nhat$ represents the outward normal vector at the boundary.
    (Note: the order of the above list corresponds to the order of boundary 
    condition precedent in code with the greater the integer, the greater the 
    precedent.)
    
    FEM begins by dividing the spatial domain $\Omega$
    into a set of elements.
    \[ \Omega = \Omega_1 \cup \Omega_2 \cup \Omega_3 \cup \ldots \cup
      \Omega_{N_E} \]
    where $\{\Omega_e\}$ for $e = 1,2,\ldots,N_E$ is a set of
    non-overlapping elements $\Omega_i \cap \Omega_j = \emptyset$ for $i \ne j$ 
    and $N_E$ is the number of elements in the problem.
    Elements are in an unstructured grid and can be generated by a general
    method to describe the geometry of the problem.
    
    Finite element derivation begins with \eref{eq:multigroup_source}.
    Proceeding with the Galerkin Finite Element Method, the equation is 
    multiplied by a testing function $v(\vr) \in H_1(\Omega)$ 
    Where $H$ is the Sobolev Space. 
    \begin{equation}
      -\grad \cdot (D_g(\vr) \grad \phi_g(\vr)) v(\vr) + 
        \Sigma_{r,g}(\vr) \phi_g(\vr) v(\vr) =
        q_g(\vr) v(\vr)
    \end{equation}
    Then, the equation is integrated over the problem domain. This integration
    yields the Weak Form or Variational Form of the problem.
    \begin{equation}
      - \int_{\Omega} \grad \cdot (D_g(\vr) \grad \phi_g(\vr)) v(\vr) \; d\Omega
        + \int_{\Omega} \Sigma_{r,g}(\vr) \phi_g(\vr) v(\vr) \;d\Omega =
        \int_{\Omega} q_g(\vr) v(\vr) \;d\Omega
    \end{equation}
    
    For the purposes of this application, material cross-sections and the
    neutron source $q_{g,e}$ are assumed to be constant within an element. 
    To calculate a constant neutron source within an element, \eref{eq:q} is
    used for the average flux in an element.
    \begin{align}
      q_{g,e} &= q_{fiss,g,e} + q_{up,g,e} + q_{down,g,e} \\
      \label{eq:qelement_fiss}
      q_{fiss,g,e} &= \frac{\chi_{g,e}}{\keff} \sum_{g'=1}^G \nu
        \Sigma_{f,g',e} \phiavg_{g',e} \\
      \label{eq:qelement_up}
      q_{up,g,e} &= \sum_{g'=g+1}^G \Sigma{s,g' \rightarrow g,e}
        \phiavg_{g',e}\\
      \label{eq:qelement_down}
      q_{down,g,e} &= \sum_{g'=1}^{g-1} \Sigma_{s,g' \rightarrow g,e}
        \phiavg_{g',e}
    \end{align}
    For first-order, linear implementations of the FEM, the exact
    element-average flux $\phiavg_{g,e}$ is
    \begin{equation}
      \label{eq:phiavg}
      \phiavg_{g,e} = \frac{1}{N_p} \sum_{i \in \Omega_e} \phi_{i,g}
    \end{equation}
    where $N_p$ is the number of solution nodes on the element. For example, a
    triangle has $N_p = 3$.

    Given material properties and neutron sources constant over the element, 
    the integral can be partitioned into a sum of integrals over the 
    elements in the domain assuming the set of elements 
    \begin{equation} \label{eq:element_by_element}
      -\sum_{e=1}^{N_E} D_{g,e} 
        \int_{\Omega_e} \grad \cdot \grad \phi_g(\vr) v(\vr) \; d\Omega_e +
        \sum_{e=1}^{N_E} \Sigma_{r,g,e} \int_{\Omega_e} \phi_g(\vr) v(\vr) 
        \;d\Omega_e = \sum_{e=1}^{N_E} q_{g,e} \int_{\Omega_e} v(\vr) 
        \; d\Omega_e
    \end{equation}
    The Second Green's Theorem is used to rewrite the integral in the first
    term. A proof can be found in \cite{textbookli} in Theorem 9.2. The Second 
    Green's Theorem is in \eref{eq:greens}.
    \begin{equation} \label{eq:greens}
      -\int_{\Omega_e} \grad \cdot \grad \phi_g(\vr) v(\vr) \;d\Omega_e =
        -\int_{\partial \Omega_e}  
        \frac{\partial \phi_g(\vr)}{\partial \nhat} v(\vr)\; ds +
        \int_{\Omega_e} \grad \phi_g(\vr) \cdot \grad v(\vr) \; d\Omega_e
    \end{equation}
    Where $\frac{\partial \phi_g(\vr)}{\partial \nhat}$ is the outward normal 
    derivative, sometimes written $\grad \phi_g(\vr) \cdot \nhat$, and the 
    integral $ds$ is a line integral in two dimensions or a surface integral in 
    three dimensions. Recognizing that this quantity will only be relevant on 
    the boundary of the problem, the value of the outward normal derivative may 
    be specified in a boundary condition. Specifically, the albedo boundary 
    condition which has the following form for $\vr \in \partial \Omega$. 
    \begin{align}
      D_g(\vr) \grad \phi_g(\vr) \cdot \nhat + \albedo \phi_g(\vr) &= 0 \\
      D_g(\vr) \grad \phi_g(\vr) \cdot \nhat &= -\albedo \phi_g(\vr)
    \end{align}
    Substituting \eref{eq:greens} into  \eref{eq:element_by_element} and 
    assuming the outward normal derivative is specified in the form of an albedo
    boundary condition with $\albedo$ constant throughout the problem.
    \begin{multline} 
      -\sum_{e=1}^{N_E} D_{g,e} \int_{\partial \Omega_e} v(\vr) 
        \frac{\partial \phi_g(\vr)}{\partial \nhat} \;ds + \sum_{e=1}^{N_E} 
        D_{g,e} \int_{\Omega_e} \grad \phi_g(\vr) \cdot \grad v(\vr) 
        \; d\Omega_e + \\
        \sum_{e=1}^{N_E} \Sigma_{r,g,e} \int_{\Omega_e} \phi_g(\vr) v(\vr) 
        \; d\Omega_e =
        \sum_{e=1}^{N_E} q_{g,e} \int_{\Omega_e} v(\vr) \; d\Omega_e
    \end{multline}
    \begin{multline} \label{eq:element_boundary}
      \sum_{e=1}^{N_E} \albedo \int_{\partial \Omega_e} v(\vr) 
        \phi_g(\vr) \;ds + \sum_{e=1}^{N_E} D_{g,e}
        \int_{\Omega_e} \grad \phi_g(\vr) \cdot \grad v(\vr) \; d\Omega_e + \\
        \sum_{e=1}^{N_E} \Sigma_{r,g,e} \int_{\Omega_e} \phi_g(\vr) v(\vr) 
        \; d\Omega_e =
        \sum_{e=1}^{N_E} q_{g,e} \int_{\Omega_e} v(\vr) \; d\Omega_e
    \end{multline}
    Next, the function of interest $\phi_g(\vr)$ is assumed to be a linear 
    combination of chosen basis functions $\{\basis_i\}$.
    \begin{equation} \label{eq:linear_combination}
      \phi_g(\vr) = \sum_{i=1}^{DOF} \beta_{g,i} \basis_i(\vr)
    \end{equation}
    Where coefficients $\{\beta_{g,i}\}$ are unknown and will be determined and $DOF$
    is the degree of freedom of the problem. Typically $DOF$ is the number of
    nodes minus any nodes for which the flux is fixed (e.g. zero-flux nodes).
    Typically, these basis functions have unit magnitude and are centered at the
    node  points so the coefficients $\beta_i$ are the approximated solution at
    the nodes. It is also convenient for basis functions to have compact
    support. That is, basis functions are created such that they are non-zero in
    only one element and zero in all other elements.
    Basis functions are typically piecewise continuous polynomials of arbitrary 
    degree. For selected elements, basis functions will be defined explicitly in
    \sref{sec:matrix_quantities}.
    Linear and quadratic polynomials are common but for the application 
    presented here, only linear basis functions are explored.

    The test function $v(\vr)$ is also chosen as a linear combination of the 
    basis functions.
    \begin{equation} \label{eq:linear_superposition}
      v(\vr) = \sum_{j=1}^{DOF} \basis_j(\vr)
    \end{equation}
    The magnitude of the testing function is arbitrary so the magnitude is fixed
    to the magnitude of the basis function.
    
    \eref{eq:linear_combination} and \eref{eq:linear_superposition} are inserted 
    into \eref{eq:element_boundary}.
    \begin{multline}
      \label{eq:this_above}
      \sum_{e=1}^{N_E} \albedo \sum_{i=1}^{N} \beta_{i,g}
        \int_{\partial \Omega_e}
        \basis_i(\vr)  \basis_j(\vr) \;ds +
        \sum_{e=1}^{N_E} D_{g,e} \sum_{i=1}^{N} \beta_{i,g}
        \int_{\Omega_e} \grad \basis_i(\vr) \cdot \grad \basis_i(\vr)\;d\Omega_e
        + \\
        \sum_{e=1}^{N_E} \Sigma_{r,g,e} \sum_{i=1}^{N} \beta_{i,g}
        \int_{\Omega_e} \basis_i(\vr) \basis_j(\vr) \; d\Omega_e =
        \sum_{e=1}^{N_E} q_{g,e} \sum_{i=1}^{N} 
        \int_{\Omega_e} \basis_i(\vr) \; d\Omega_e
    \end{multline}
    Combining all elements, \eref{eq:this_above} can be rearranged as a linear
    system of equations.
    \begin{multline}
      \sum_{i=1}^{N} \beta_{i,g} \sum_{j=1}^{N} \left(
        \sum_{e=1}^{N_E} \albedo \int_{\partial \Omega_e}
        \basis_i(\vr)  \basis_j(\vr) \;ds +
        \sum_{e=1}^{N_E} D_{g,e} 
        \int_{\Omega_e} \grad \basis_i(\vr) \cdot \grad \basis_j(\vr)\;d\Omega_e
        \right.
        + \\
        \left.
        \sum_{e=1}^{N_E} \Sigma_{r,g,e}
        \int_{\Omega_e} \basis_i(\vr) \basis_j(\vr) \; d\Omega_e \right) =
        \sum_{i=1}^{N} \left(
        \sum_{e=1}^{N_E} q_{g,e} 
        \int_{\Omega_e} \basis_i(\vr) \; d\Omega_e \right)
    \end{multline}
    Which can be written in the notation common to the FEM
      \begin{equation}
        \label{eq:fem_notation}
        a(\basis_i,\basis_j) = f(\basis_i)
      \end{equation}
    where $a(\basis_i,\basis_j)$ is the bilinear form and $f(\basis_i)$ is the 
    linear form of the finite element system.
    Or in matrix format as
    \begin{equation}
      \label{eq:matrix_notation}
      \ma \vu = \vf
    \end{equation}
    The notation of \eref{eq:fem_notation} is common to mathematical discussions
    of the FEM. 
    
    The diffusion coefficient $D_g(\vr)$ is non-zero and bounded and
    the removal cross-section $\Sigma_{r,g}(\vr)$ is bounded. The Lax-Milgram 
    Lemma implies the solution to the FEM as derived here is both unique and 
    bounded (See Theorem 9.3 \cite{textbookli}). This is 
    not the entire description as the source function $q_g(\vr)$ is updated on 
    each power iteration. What the satisfaction of the Lax-Milgram Lemma does 
    imply in this instance is that for a fixed source on a given problem 
    iteration, a unique solution exists. The global problem remains an 
    eigenvalue problem as described in \cite{duderstathamilton}. The power
    iteration method will 
    return the fundamental eigenvalue $\keff$ and the fundamental 
    eigenmode $\phi$ which may be normalized by an arbitrary constant to be
    discussed in depth in \sref{sec:power_iterations}.
    
    In matrix notation \eref{eq:matrix_notation}, matrix $\ma$ is 
    described by integral quantities, vector $\vu$ is the unknown magnitudes of
    the basis functions $\{\beta_{g,i}\}$, and vector $\vf$ is described by the 
    source integral quantity. Inspecting the matrix $\ma$ and the vector $\vf$
    reveals the following.
    \begin{align}
      \label{eq:matrix_population}
      A_{i,j,g,e} &= \albedo \int_{\partial \Omega_e} \basis_i(\vr) 
        \basis_j(\vr) \; ds + D_{g,e} 
        \int_{\Omega_e} \grad \basis_i(\vr) \cdot \grad \basis_j(\vr) \;
        d\Omega_e + \Sigma_{r,g,e} \int_{\Omega_e} \basis_i(\vr) \basis_j(\vr)
        \; d\Omega_e \\
      \label{eq:vector_population}
      f_{i,g,e} &= q_{g,e} \int_{\Omega_e} \basis_i(\vr) \;d\Omega_e
    \end{align}
    Then, 
    \begin{align}
      A_{i,j,g} &= \sum_{e=1}^{N_E} A_{i,j,g,e} \\
      f_{i,g} &=  \sum_{e=1}^{N_E} f_{i,g,e}
    \end{align}
    which leads to the natural population of the matrix $\ma$ on an 
    element-by-element basis. That is, the matrix $\ma$ is assembled by looping
    through all of the elements and summing their contribution to the matrix. 
    Note that the contribution due to the surface integral will be zero in 
    elements not on the boundary and may also be zero for problems with select
    boundary conditions. See \sref{sec:boundary_conditions} for boundary
    condition discussion.  The population of the vector $\vf$ is done similarly. 
    Then, the matrix $\ma$ and the vector $\vf$ are known for each energy group.
    The equations are solved for one group at a time and $\phi_g$ is calculated
    and stored.
    
    Though the notation may be obtuse, the above reduces to a linear system of
    equations. These equations are constructed from the integral quantities 
    specified by the FEM and the coefficients given by the cross-sections and
    fixed source regions. The integral quantities themselves are expressed 
    explicitly in the next section.
    
  \subsection{Matrix Quantities}
    \label{sec:matrix_quantities}
    For certain simple elements, the integral quantities described in 
    \eref{eq:matrix_population} and \eref{eq:vector_population} have exact 
    analytic forms. For this application, linear triangles and linear wedges
    are investigated and many of the integrals have exact expressions. If these 
    quantities cannot be expressed exactly, or doing so would be computationally
    difficult, quadratures can be used. For certain problems, these 
    quadratures can express the integrals exactly. This will be discussed in 
    \sref{sec:quadratures}.
    \subsubsection{Linear Triangles}
      Linear triangles are common to two-dimensional finite element methods and
      have been investigated in many applications \cite{Hosseini2017} 
      \cite{Hosseini2013} \cite{Hosseini2015}. The linear triangle element is a
      triangle  defined by three corner coordinates with basis functions located 
      on each corner. A triangle element is sketched in 
      \fref{fig:sketch_triangle}.
      \begin{figure}
        \centering
        \includegraphics[width=0.3\textwidth]{sketch_triangle}
        \caption{Description of Triangle Element.}
        \label{fig:sketch_triangle}
      \end{figure}
      \begin{figure}
        \centering
        \includegraphics[width=0.4\textwidth]{Tref}
        \caption{Description of Reference Triangle.}
        \label{fig:Tref}
      \end{figure}
      It is difficult to analytically calculate the desired integral quantities
      for an arbitrary triangle. Instead, a simplified reference element is
      created and quantities are calculated for the reference element and then
      translated to the arbitrary element using a Jacobian. 
      The reference triangle $T_{ref}$ is located in
      $\xi \in [0,1]$ and $\eta \in [0,1-\xi]$ as shown in \fref{fig:Tref}. The
      basis functions are zero outside of the reference triangle and within the
      reference triangle, basis functions for the are provided in the
      coordinates of $T_{ref}$ (see \fref{fig:Tref}).
      \begin{align}
        \basis_i(\xi,\eta) &= 0 \; \forall \; (\xi,\eta) \notin T_{ref} \\
        \basis_1(\xi,\eta) &= \xi \\
        \basis_2(\xi,\eta) &= \eta \\
        \basis_3(\xi,\eta) &= 1-\xi-\eta
      \end{align}
      
      There are simple expressions
      for the integral quantities for an arbitrary triangle that are originally
      proposed in \cite{textbookwhite}. The expression for
      the line integral is found in \cite{computerLab}. For a triangle with 
      corners $\{ x_i,y_i \}$ with $i=1,2,3$.
      \begin{align}
        \int_{\Omega_e} \grad \basis_i(\vr) \cdot \grad \basis_j(\vr) 
          \;d\Omega_e &= \frac{1}{4 A_e}
          ((x_{i+1}-x_{i+2})(x_{j+1}-x_{j+2}) + 
          (y_{i+1}-y_{i+2})(y_{j+1}-y_{j+2})) \\
        \int_{\Omega_e} \basis_i(\vr) \basis_j(\vr) \;d\Omega_e &= 
          \frac{A_e}{12} (1+\delta_{ij}) \\
        \int_{\Omega_e} \basis_i(\vr) \;d\Omega_e &= \frac{A_e}{3} \\
        \int_{\partial \Omega_e} \basis_i(\vr) \basis_j(\vr) \;ds &=
          \frac{L_e}{6}(1+\delta_{ij}) \\
      \end{align}
      Where $A_e$ is the area of the triangular element, $L_e$ is the length of 
      the edge between node $i$ and node $j$, and $\delta_{ij}$ is the Kronecker
      delta. The area of a triangle in three dimensions is calculated for a
      triangle with corner coordinates $\vc_i = (x_i, y_i, z_i)$ with $i=1,2,3$.
      That is, $\vc_i$ is the coordinates of corner $i$.
      \begin{align}
        \va &= \vc_2 - \vc_1 \\
        \vb &= \vc_3 - \vc_1 \\
        A_e &= \half \lvert \va \times \vc \rvert \\
        \label{eq:area_triangle}
        A_e &= \half \sqrt{ (a_2 b_3 - a_3 b_2)^2 + (a_3 b_1 - a_1 b_3)^2 +
          (a_1 b_2 - a_2 b_1)^2}
      \end{align}
      where $a_i$ is the $i^{th}$ component of vector $\va$ and $b_i$ is the
      $i^{th}$ component of vector $\vb$.
      The Kronecker delta is defined as
      \begin{equation} \label{eq:kroneker_delta}
        \delta_{ij} =
        \begin{cases}
          0 & \text{if } i \ne j \\
          1 & \text{if } i = j
        \end{cases}
      \end{equation}
      For higher order triangular elements, it may be necessary to employ a 
      quadrature to calculate the integrals.
    \subsubsection{Linear Wedges}
      A Wedge element is a pentahedron with six corner nodes, and is sometimes
      referred to as a triangular prism. A simple example of a wedge is an
      extruded triangle. However, unlike an extruded triangle, the exact 
      geometric relation of corner nodes in a wedge is not fixed and the nodes 
      are free to expand and distort. All corner nodes are free to move
      independently of other nodes. An example of a typical and a distorted
      wedge element is shown in \fref{fig:sketch_wedge}. These elements are
      unique because there are two different types of faces. Three faces are
      quadrilateral and two are triangular. 

      \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{sketch_wedge}
        \caption{Description of Typical (left) and Distorted (right) Wedge 
          Elements.}
        \label{fig:sketch_wedge}
      \end{figure}

      In this application,
      wedges are useful to describe hexagonal geometries such as fast spectrum
      reactors. Fast reactors are typically hexagonal-z geometry so wedge
      elements are a natural choice for this coordinate system. In addition,
      wedges can build geometries in a manner similar to triangular
      elements. Reactor geometries are also typically described in lattices so
      the wedge element allows for easily ``stacking'' lattices on top of each
      other.

      \begin{figure}
        \centering
        \includegraphics[width=0.5\textwidth]{Wref}
        \caption{Description of Reference Wedge.}
        \label{fig:Wref}
      \end{figure}

      The reference wedge $W_{ref}$ is located in 
      $\xi \in [0,1]$, $\eta \in [0,1-\xi]$, and $\zeta \in [-1,1]$. The
      coordinate system of the reference wedge is shown in \fref{fig:Wref}. The
      basis functions are, again, zero outside of the reference wedge and are 
      provided within the reference wedge.
      \begin{align}
        \basis_i(\xi,\eta,\zeta) &= 0 \; \forall \; (\xi,\eta,\zeta)
          \notin W_{ref} \\
        \basis_1(\xi,\eta,\zeta) &= \half (1-\zeta)(1-\xi-\eta) \\
        \basis_2(\xi,\eta,\zeta) &= \half (1-\zeta)\xi \\
        \basis_3(\xi,\eta,\zeta) &= \half (1-\zeta)\eta \\
        \basis_4(\xi,\eta,\zeta) &= \half (1+\zeta)(1-\xi-\eta) \\
        \basis_5(\xi,\eta,\zeta) &= \half (1+\zeta)\xi \\
        \basis_6(\xi,\eta,\zeta) &= \half (1+\zeta)\eta 
      \end{align}

      The integrals of the basis function over the element are given in
      \eref{eq:bigmat} through \eref{eq:wedge_surface_integral}. The values
      presented herein are not found in literature and have been calculated by
      the author
      \begin{align}
        \label{eq:bigmat}
        \int_{\Omega_e} \basis_i(\vr) \basis_j(\vr) \;d\Omega_e &= 
          \frac{V_e}{144}
          \begin{pmatrix}
            4 & 2 & 2 & 2 & 1 & 1 \\
            2 & 4 & 2 & 1 & 2 & 1 \\
            2 & 2 & 4 & 1 & 1 & 2 \\
            2 & 1 & 1 & 4 & 2 & 2 \\
            1 & 2 & 1 & 2 & 4 & 2 \\
            1 & 1 & 2 & 2 & 2 & 4 
          \end{pmatrix} \\
        \int_{\Omega_e} \basis_i(\vr) \;d\Omega_e &= \frac{V_e}{12} \\
        \label{eq:wedge_surface_integral}
        \int_{\partial \Omega_e} \basis_i(\vr) 
          \basis_j(\vr) \;d\partial\Omega_e &= 
          \begin{cases}
            \frac{A_{\Delta}}{12}(1+\delta_{ij})&\text{if triangular surface} \\
            \frac{A_{\Box}}{36}(1+\delta_{ij})(1-\half \delta_{i,(5-j)}) &
              \text{if quadrilateral surface}
          \end{cases}
      \end{align}
      Where $V_e$ is the volume of the element, $A_{\Delta}$ is the area of the
      triangular surface, and $A_{\Box}$ is the area of the quadrilateral
      element. $A_{\Delta}$ is computed according to \eref{eq:area_triangle} and
      $A_{\Box}$ is computed as the sum of the area of two triangles, employing
      the same formula.
      
      For a simple extruded triangle,
      the volume calculation is straightforward and is the product of triangular
      area and linear height. However, allowing the nodes to
      move with respect to each other makes the volume of the element difficult
      to calculate. Therefore, the Jacobian is used to calculate $V_e$. For more
      detail, see \sref{sec:quadratures}, especially \tref{tab:jacobi}.

      The matrix in \eref{eq:bigmat} is indexed $M_{ij}$
      and is presented as a matrix because of its irregular form. Notice the 
      integral containing the gradient operator has been omitted because it is
      computed using a quadrature. If it could be computed analytically, it 
      would be less computationally efficient than using a quadrature.

  \subsection{Quadratures}
    \label{sec:quadratures}
    Quadratures are sets of coordinates and weights which allow for the exact 
    integration of polynomials of given order. For a given set of weights 
    $\{w_i\}$ and a set of coordinates $\{\vx\}$, an integral can be represented
    as the sum
    \begin{equation}
      \label{eq:quadrature}
      \int_{\Omega} f(\vx) \;d\Omega \approx \sum_{i=1}^{N} w_i f(\vx_i)
    \end{equation}
    where $\Omega$ is an arbitrary domain described by $\{\vx_i\}$. The 
    quadratures in \eref{eq:quadrature} will exactly integrate a polynomial of
    the order of the quadrature. It is not necessarily true that $N$ be the 
    order of the quadrature.
    
    For one-dimensional integrals, the Gaussian quadrature is common and the 
    most compact quadrature. The Gaussian quadrature exactly integrates a 
    polynomial of order $n$ using exactly $n$ points. Weights and coordinates
    for this quadrature set are presented in \cite{gaussianQuadrature}. These
    are standard values and can be easily calculated by the definition of the
    quadrature. For this quadrature, $n=N$.
    
    Two-dimensional and three-dimensional quadratures are necessary for the 
    FEM. Triangular quadratures are not as simple to derive
    as line quadratures and the number of points need not equal the order of the
    polynomial integrated. The triangular quadrature as implemented here is 
    symmetric and open. That is, there are no points on the boundary of the 
    triangle. The weights and coordinates for this triangular quadrature are 
    found in \cite{triangleQuadrature}. Any triangular quadrature will suffice
    that exactly integrates polynomials of a given order.
    
    Quadrilateral quadrature sets are simply tensor products of two line 
    Gaussian quadratures. For an order $N$ polynomial, now $n^2$ points are 
    required. 
    
    Wedge quadrature sets are simply tensor products of a line Gaussian 
    quadrature and a triangular quadrature. 
    
    Basis functions are polynomials of first, second, or third order. These 
    quadratures are capable of exactly integrating functions of given order so 
    there is a quadrature order that will exactly integrate the finite element 
    quantities to the precision of the quadrature and the numeric precision. The
    table of the order required for exact integration are provided in 
    \tref{tab:quadrature_orders}.

    \begin{table}
      \begin{center}
        \caption{Quadrature Orders for FEM Quantities.}
        \label{tab:quadrature_orders}
        \begin{threeparttable}
          \begin{tabular}{lccc}
            \toprule
            Quantity & Linear & Quadratic & Cubic \\
            \midrule
            $\int_{\Omega} \basis_i(\vr) \;d\Omega$ & 1 & 2 & 4
              \tnote{$\dagger$} \\
            $\int_{\Omega} \basis_i(\vr) \basis_j(\vr) \;d\Omega$ &
              2 & 4 & 6 \\
            $\int_{\Omega} \grad \basis_i(\vr) \cdot \grad \basis_j(\vr) 
              \;d\Omega$ & 2 & 3 & 5 \\
            \bottomrule
          \end{tabular}
          \begin{tablenotes}
            \item[$\dagger$] A third order quadrature would be exact but the 
              quadrature would have negative weights so a fourth order 
              quadrature is selected.
          \end{tablenotes}
        \end{threeparttable}
      \end{center}
    \end{table}
    
    All of the quadratures described here are tabulated for a reference element
    be it a line, triangle, quadrilateral, or wedge. Integration in the 
    FEM is performed on an arbitrary element in space. Therefore, it is 
    necessary to perform a coordinate transform when using a quadrature set.
    % get VERY explicit here so someone else can actually solve the problem...
    \begin{equation}
      \int_{\Omega} f(\vx) \;d\Omega = 
        \int_{\Omega_{ref}} f(\vx) \lvert \mj \rvert \;d\Omega_{ref} \approx
        \sum_{i=1}^{N} w_i f(\vx_i) \lvert \mj_i \rvert
    \end{equation}
    where $\mj$ is the Jacobian matrix, $\mj_i$ is the Jacobian matrix at 
    quadrature coordinate $\vx_i$, and $\lvert \cdot \rvert$ represents
    the matrix determinant. Notationally, $J=\lvert \mj \rvert$ and is termed
    the Jacobi.

    For isoparametric elements, the Jacobi is constant over the element and can
    be precalculated to save from populating, and evaluating the 
    determinant of a matrix for each integration point. For the elements of 
    concern, these values are presented in \tref{tab:jacobi} as found in 
    \cite{textbookcolorado}.
    \begin{table}
      \caption{Jacobi for Selected Elements.}
      \label{tab:jacobi}
      \begin{center}
        \begin{tabular}{ll}
          \toprule
          Element & $J$ \\
          \midrule
          Triangle      & $A_e$ \\
          Quadrilateral & $\frac{1}{4} A_e$ \\
          Wedge         & $\half V_e$ \\
          \bottomrule
        \end{tabular}
      \end{center}
    \end{table}

    For the general element, the Jacobian matrix, $\mj$, is calculated at each
    point of the quadrature $(x_i,y_i,z_i)$ as described in
    \eref{eq:calcJacobian}.
    \begin{equation}
      \label{eq:calcJacobian}
      \mj = 
      \begin{pmatrix}
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \xi} x_k   &
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \zeta} x_k &
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \eta} x_k   \\
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \xi} y_k   &
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \zeta} y_k &
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \eta} y_k   \\
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \xi} z_k   &
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \zeta} z_k &
        \sum_{k=1}^P \frac{\partial \basis_k}{\partial \eta} z_k   \\
      \end{pmatrix}
    \end{equation}
    Where $P$ is the number of points in the element, $\basis_k$ is the basis
    function centered at the $k^{th}$ corner, and for corner cordinates 
    $\{x_k,y_k,z_k\}$ for $k = 1,2,\ldots,P$.

    With the Jacobian calculated, $J=\lvert\mj\rvert$ is simply the matrix 
    determinant. For the quadrature integration of derivative quantities as
    necessary in the FEM, the derivatives must also be translated from the
    reference element to the spatial element. This is performed according to 
    \algorithmref{algorithm:deriv_int}. The notation is dense as the method
    requires two sets of coordinates. First, the coordinate in the reference
    element $(\xi,\zeta,\eta)$ and second, the coordinate in Cartesian space
    $(x,y,z)$.
    Then, the vector $\vd_{i,\,(\xi,\zeta,\eta)}$ is the gradient vector for
    $\basis_i$ with respect to the reference coordinates $(\xi,\zeta,\eta)$.
    Vector $\vd_{i,\,(x,y,z)}$ is the gradient vector for $\basis_i$ with
    respect to the Cartesian coordinates $(x,y,z)$. In 
    \algorithmref{algorithm:deriv_int}, the quadrature has points $\{\vx_p\}$ 
    and weights $\{w_p\}$ for $p = 1,2,\ldots,N_P$ and the value of the integral
    is represented by \eref{eq:deriv_integral}.
    \begin{equation}
      \label{eq:deriv_integral}
      v = \int_{\Omega_e} \grad \basis_i(\vr) \cdot \grad \basis_j(\vr) \;
      d\Omega_e
    \end{equation}

    \begin{algorithm}
      \caption{Integral of Derivative with Jacobian Method.}
      \label{algorithm:deriv_int}
      \begin{algorithmic}[1]
        \State $v=0$
        \For{$p=1,N_P$}
          \State Calculate the Jacobian $\mj$ as in \eref{eq:calcJacobian}.
          \State Calculate the vector $\vd_{i,\,(\xi,\zeta,\eta)}$ at quadrature
            point $\vx_p$.
          \State Calculate the vector $\vd_{j,\,(\xi,\zeta,\eta)}$ at quadrature
            point $\vx_p$.
          \State Invert and store the Jacobian $\mj^{-1}$.
          \State Calculate the vector $\vd_{i,\,(x,y,z)} =
            \vd_{i,\,(\xi,\zeta,\eta)} \mj^{-1}$.
          \State Calculate the vector $\vd_{j,\,(x,y,z)} =
            \vd_{j,\,(\xi,\zeta,\eta)} \mj^{-1}$.
          \State $v = v + \left(\vd_{i,\,(x,y,z)}^{T} \vd_{j,\,(x,y,z)}\right)
            \, w_p \, \lvert \mj \rvert$
        \EndFor
      \end{algorithmic}
    \end{algorithm}

\section{Power Iterations}
  \label{sec:power_iterations}
  The FEM is used to solve a fixed source problem for a given source
  distribution $q_g(\vr)$. However, for multigroup problems with scattering, 
  the problem is not a fixed-source problem as the source is not known
  explicitly due to interaction between groups. For eigenvalue problems, the
  problem is also not a fixed-source problem and has many solutions. The method 
  of Power Iterations allows these problem to be solved iteratively and in the
  case of the eigenvalue problem, to solve for the fundamental eigenmode.

  \subsection{Convergence of Power Iteration Method}
    Noting the FEM equations can be written as matrix form
    \eref{eq:matrix_notation}, the finite diffusion equation can be rewritten as
    shown in \cite{gehinThesis}.
    \begin{equation}
      \label{eq:gehin_notation}
      \mb(\vPhi,\keff) \vPhi = \frac{1}{\keff} \mm \vPhi
    \end{equation}
    where $\vPhi$ is the vector of the flux containing all energy groups, $\mb$
    contains the diffusion, removal, and all scattering terms, and $\mm$ 
    includes all fission generation. $\mb$ is an S-matrix and its inverse, 
    $\mb^{-1}$ exists and has all positive elements \cite{nakamura}. Therefore,
    \eref{eq:gehin_notation} can be rewritten as
    \begin{equation}
      \label{eq:gehin_solution}
      \vPhi = \frac{1}{\keff} \mr \vPhi
    \end{equation}
    where
    \begin{equation}
      \label{eq:gehin_r}
      \mr = \mb^{-1} \mm.
    \end{equation}
    Matrix $\mm$ is non-symmetric and is non-negative; therefor, $\mr$ is a
    non-symmetric, non-negative matrix.

    In the solution of \eref{eq:multigroup_diffusion}, the largest eigenvalue,
    $\keff$, is desired. The solution can be found using the power method which 
    can be written as 
    \begin{align}
      \label{eq:power_iteration_phi}
      \vPhi^{(s+1)} &= \frac{1}{\keff^{(s)}} \mr \vPhi^{(s)} \\
      \label{eq:power_iteration_eigenvalue}
      \keff^{(s+1)} &= \keff^{(s)} \frac{\vw^{T} \vPhi^{(s+1)}}
        {\vw^{T} \vPhi^{(s)}} \qquad s = 1,2,\ldots,\infty
    \end{align}
    where $s$ is the iteration counter, $\vw$ is the weighting vector and 
    $\vw^{T} \vPhi$ is the vector inner-product. According to the 
    Perron-Frobenius theorem, a matrix with the properties of $\mr$ has a 
    unique, positive eigenvalue greater in magnitude than the modulus of all 
    other eigenvalues of the matrix. The weighting vector $\vw$ is arbitrary 
    but does affect convergence rate. For this application 
    $\vw = \{\nu \Sigma_f\}$ such that the inner product 
    $\{\nu \Sigma_f\}^T \vPhi$ represents the summation of the fission source 
    throughout all energy groups and all elements. 
    
    It can then be shown in that the power method
    described in \eref{eq:power_iteration_phi} and
    \eref{eq:power_iteration_eigenvalue}
    along with matrix $\mr$ converges to the largest eigenvalue, $\keff$, and
    unique positive eigenvector \cite{nakamura}. For notation and enumeration,
    allow the eigenvalue to be rewritten as
    \begin{equation}
      \label{eq:lambda}
      \lambda = \frac{1}{\keff}.
    \end{equation}
    The eigenvectors $\vu_n$ and
    corresponding eigenvalues $\lambda_n$ of $\mr$ are defined by
    \begin{equation}
      \label{eq:eigen_definition}
      \vu_n = \lambda_n \, \mr \, \vu_n
    \end{equation}
    It may be proved that all eigenvalues $\lambda$ are real, positive, and 
    distinct. The eigenvalues are then numbered in the sequence:
    \begin{equation}
      \label{eq:eigen_order}
      \lambda_0 < \lambda_1 < \lambda_2 < \cdots < \lambda_I
    \end{equation}
    where $I$ is the rank of the problem. The eigenvectors have the 
    orthogonality relations:
    \begin{equation}
      \label{eq:eigen_orthogonality}
      \vu^T_n \vu= 0  \qquad \text{for } n \ne m
    \end{equation}
    where $\Phi^T \Phi$ is the scalar inner-product. Assume eigenvectors are
    normalized such that:
    \begin{equation}
      \label{eq:eigen_normalization}
      \vu_n^T \vu_n = 1 \qquad \text{for } n = 1, 2, \ldots, I
    \end{equation}
    The initial vector $\Phi^{(0)}$ may be expressed as a projection onto the
    eigenvectors using a linear superposition of eigenvectors.
    \begin{equation}
      \label{eq:eigen_projection}
      \Phi^{(0)} = \sum_n c_n^{(0)} \vu_n
    \end{equation}
    where $c_n^{(0)}$ is a coefficient given by the orthogonality relationship
    from \eref{eq:eigen_orthogonality}.
    \begin{equation}
      \label{eq:eigen_cn}
      c_n^{(0)} = \vu_n^T \Phi^{(0)}
    \end{equation}
    Using this eigenmode projection, \eref{eq:power_iteration_phi} can be
    rewritten as 
    \begin{equation}
      \label{eq:phi_expand_begin}
      \Phi^{(s+1)} = \lambda^{(s)} \, \lambda^{(s-1)} \, \lambda^{(s-2)} \, 
        \ldots \, \lambda^{(0)} \, \mr \phi^{(0)}.
    \end{equation}
    Then, \eref{eq:eigen_projection} can be inserted into
    \eref{eq:phi_expand_begin}.
    \begin{align}
      \Phi^{(s+1)} &= \lambda^{(s)} \, \lambda^{(s-1)} \, \ldots \, 
        \lambda^{(0)} \sum_n^I c_n^{(0)} \, \mr \, \vu_n \\
      &= \left( \prod_{p=0}^{s} \lambda^{(p)} \right) \sum_n^I c_n^{(0)} \, 
        \mr \, \vu_n
    \end{align}
    Recalling the relationship \eref{eq:eigen_projection}
    \begin{equation}
      \label{eq:phi_expand_above}
      \Phi^{(s+1)} = \left( \prod_{p=0}^s \lambda^{(p)} \right) 
        \sum_n^I c_n^{(0)} \frac{1}{\lambda_n^{(s+1)}} \vu_n
    \end{equation}
    \eref{eq:phi_expand_above} can be rewritten by dividing and multiplying by
    $\lambda_0$ and dividing and multiplying by $c_0$.
    \begin{align}
      \Phi^{(s+1)} &= \left( \prod_{p=0}^s \frac{\lambda^{(p)}}{\lambda_0} 
        \right) c_0 \left( \vu_0 \sum_n^I \frac{c_n^{(0)}}{c_0^{(0)}}
        \frac{\lambda_0}{\lambda_n^{(s+1)}} \vu_n \right) \\
      \label{eq:eigen_phi_converge}
      \Phi^{(s+1)} &\approx \text{const.} \left( 
        \vu_0 \sum_n^I \frac{c_n^{(0)}}{c_0^{(0)}}
        \frac{\lambda_0}{\lambda_n^{(s+1)}} \vu_n \right)
    \end{align}
    The ordering of unique eigenvalues required by \eref{eq:eigen_order} 
    requires $\lambda_0 / \lambda_n < 1$ and the problem is convergent. 
    Specifically, \eref{eq:eigen_phi_converge} converges to a constant times 
    $\vu_0$. The convergence rate is determined by the dominance ratio
    \begin{equation}
      \label{eq:dominance_ratio}
      d \equiv \max_{n>0} \frac{\lambda_0}{\lambda_n} =
        \frac{\lambda_0}{\lambda_1}
    \end{equation}
    and it can be seen that for problems with small dominance ratio, the power
    iteration method will converge more quickly. Convergence criteria are then
    specified as an absolute tolerance in the sense of the eigenvalue
    \begin{equation}
      \label{eq:eigenvalue_tol}
      \epsilon_{\lambda} > | \lambda^{(s+1)} - \lambda^{(s)} |
    \end{equation}
    and as a relative tolerance in the sense of the eigenvector
    \begin{equation}
      \label{eq:eigenvector_tol}
      \epsilon_{\Phi} > \max_i \left| \frac{\Phi_i^{(s+1)} - \Phi_i^{(s)}}
        {\Phi_i^{(s)}}\right| .
    \end{equation}

    It is important to note that all of the analysis in this section assumed the
    matrix $\mr$ does not change between iterations. For simple multigroup
    criticality calculations, this assumption is correct. However, in realistic
    power reactor simulations, the cross-sections of the problem may be 
    considered functions of material temperature or may have some changing
    number density. For these problems, the matrix $\mr$ is necessarily updated
    on each power iteration in a nonlinear manner. For nonlinear power
    iteration updates, convergence is no longer guaranteed. The argument for
    convergence with nonlinear power iterations in this application falls back
    to the stability of the physical system.
    
  \subsection{Calculation of Source with Power Iterations}

    The multigroup neutron diffusion equation \eref{eq:multigroup_source} can be
    written with the source term $q_g(\vr)$ expanded into its component parts.
    \begin{equation} \label{eq:multigroup_source_expand}
      -\grad \cdot (D_g(\vr) \grad \phi_g^{(s)}(\vr)) + \Sigma_{r,g}(\vr)
      \phi_g^{(s)}(\vr) = q_{fiss,g}(\vr) + q_{up,g}(\vr) + q_{down,g}(\vr)
    \end{equation}
    Recall from the definitions of the source components that their calculation
    requires the flux $\phi_g(\vr)$. The source components each require
    different energy groups of the flux distribution to be known. The fission
    component $q_{fiss,g}(\vr)$ requires all groups. The up-scatter component
    $q_{up,g}(\vr)$ requires lower energy groups (i.e. $g' > g$). The
    down-scatter component $q_{down,g}(\vr)$ requires higher energy groups (i.e.
    $g' < g$). Based on these requirements, these source components can be
    calculated based on different power iterations of $\phi_g(\vr)$. This is
    described in \algorithmref{algorithm:general}.
    \eref{eq:multigroup_source_expand} is then more explicitly written.
    \begin{equation} \label{eq:multigroup_power_iterations}
      -\grad \cdot (D_g(\vr) \grad \phi_g^{(s)}(\vr)) + \Sigma_{r,g}(\vr)
      \phi_g^{(s)}(\vr) = q_{fiss,g}^{(s-1)}(\vr) + q_{up,g}^{(s-1)}(\vr) +
      q_{down,g}^{(s)}(\vr)
    \end{equation}

\section{Implementation}
  A FEM neutron diffusion solution has been developed using 
  the above formulae. The program begins with a geometry description specified
  in a plain text VTK file \cite{vtk}. The VTK format is chosen because it is a
  standard that can be used with visualization tools such as ParaView
  \cite{ParaView} and VisIt \cite{VisIt}. Additionally, open-source C and Python
  packages exist for easy manipulation of the format. Cross sections are 
  specified in either a plain text user format or the ISOTXS format as common to 
  fast reactor applications and the multigroup cross-section generator \mcc 
  \cite{mcc}. The multigroup neutron diffusion equation is solved according to 
  \eref{eq:multigroup_source}. The resulting effective neutron multiplication 
  factor, $\keff$, is written to an output file. The multigroup neutron flux is
  written to a different results VTK file for easy visualization.

  \subsection{Algorithm}
    The algorithm for the solution to the diffusion equation is similar to most
    implementations of the multigroup neutron diffusion method. The algorithm 
    itself is presented in \algorithmref{algorithm:general}. The steps unique 
    to the FEM are steps \ref{state:fem_matrix} and \ref{state:fem_vector}. 
    These require the quantities previously derived and form the linear system 
    described by the FEM. 
    
    In step \ref{state:rcm} the matrix is reordered. Mathematically this has no
    effect on the result as the linear system represented is equivalent. This 
    choice to reorder the system is made to improve computational efficiency. 
    Indexing nodes that are physically proximate with proximate indices causes 
    rows in the finite element matrix $\ma$ to be closely coupled to nearby
    rows. In a general unstructured and unordered mesh rows may be coupled to 
    other random rows in the matrix. This step of reordering the matrix $\ma$ 
    seeks to decrease the bandwidth of the matrix and encourage cache hits when
    accessing coupled values in the linear system. The ordering chosen is the
    Reverse Cuthill-McKee (RCM) method and common to sparse linear systems and 
    described in \cite{rcm}.
    
    \begin{algorithm}
      \caption{General Iteration Scheme}
      \label{algorithm:general}
      \begin{algorithmic}[1]
      \State Read mesh from VTK.
      \State Initialize $\phiavg^{(0)}$.
      \State Order the nodes of the mesh into RCM order.
        \label{state:rcm}
      \State Calculate $\Sigma_s$, $\Sigma_t$, and $\nu \Sigma_f$ for each 
        element.
      \State Calculate finite element matrix $\ma_g$ for each group. Store this. 
        \label{state:fem_matrix}
      \While{Power Iteration}
        \State Update the iteration counter. $s=s+1$
        \State Update $q_{fiss,g}$ and $q_{up,g}$ for all groups from previous 
          data $\phiavg^{(s-1)}$.
        \State Update $\chi$ in each element using previous data.
          \label{state:chi_collapse}
        \For{$g=1,G$}
          \State Update $q_{down,g}$ from current data $\phiavg^{(s)}$
          \State Calculate total effective source in each element.
          \State Update finite element Vector $\vf_g$ with new source.
            \label{state:fem_vector}
          \State Solve $\ma \vu = \vf$ using an iterative technique (See
            \sref{sec:linear_system_solution}).
          \State Parse $\vu$ for $\phi$ nodal solution.
          \State Calculate element-average $\phiavg$.
        \EndFor
        \State Update $\keff$.
        \State Check convergence.
      \EndWhile
      \end{algorithmic}
    \end{algorithm}

    State \ref{state:chi_collapse} normalizes $\chi$ to preserve the 
    neutron production rate due to fission in each group in each element of the
    problem recalling the definition of $q_{fiss,g}$ as provided in
    \eref{eq:qfiss}. Typically, reactor materials are described isotopically and 
    $\chi$ may be specified isotopically. However, for calculating the fission 
    neutron source $q_{fiss,g}$, an effective $\chi$ for the element is needed. 
    From an isotopic description, $q_{fiss,g}$ is given in 
    \eref{eq:isotopic_chi}.
    \begin{equation}
      \label{eq:isotopic_chi}
      q_{fiss,g}(\vr) = \sum_{i=1}^{N_I} \widetilde{\chi_{i,g}}(\vr)
        \nu \Sigma_{f,i,g}(\vr) \phi_g(\vr)
    \end{equation}
    Where $\widetilde{\chi_{i,g}}$ is the isotopic $\chi$ and $N_I$ is the 
    number of isotopes at position $\vr$. Next, require $q_{fiss,g}$ to have the 
    form of
    \eref{eq:element_chi}.
    \begin{equation}
      \label{eq:element_chi}
      q_{fiss,g}(\vr) = \chi_g(\vr) \, \sum_{i=1}^{N_I} \nu \Sigma_{f,i,g}(\vr)
        \phi_g(\vr)
    \end{equation}
    Setting \eref{eq:isotopic_chi} equal to \eref{eq:element_chi} yields the
    expression for $\chi_g$ based on isotopic data.
    \begin{equation}
      \label{eq:chi_collapse}
      \chi_g(\vr) = \frac{\sum_{i=1}^{N_I} \widetilde{\chi_{i,g}}(\vr)
        \nu \Sigma_{f,i,g}(\vr) \phi_g(\vr)}
        {\sum_{i=1}^{N_I} \nu \Sigma_{f,i,g}(\vr) \phi_g(\vr)}
    \end{equation}
    Noting that \eref{eq:chi_collapse} requires the solution $\phi_g(\vr)$. This
    means that $\chi_g$ must be calculated for each element for each power
    iteration.
    
    A benefit of this implementation is that the finite element vector $\vf$ 
    must be updated on each power iteration of the solution whereas the matrix 
    $\ma$ is described entirely by geometry and the material cross-sections. For
    this reason, $\ma$ can be generated once at the beginning of the problem and 
    stored for the duration of the calculation.
    \FloatBarrier % make sure the algorithm is in the correct section

  \subsection{Memory and Storage}
    The finite element matrix $\ma$ is large and sparse so a sparse storage and
    sparse solution to the linear system are required. Many sparse matrix 
    implementations have been described and implemented in the past including
    triplet storage, reduced column, and reduced row storage \cite{sparseBLAS}.
    For this application a \twotable method is chosen which was uniquely 
    developed. \twotable storage is not designed to be the most efficient 
    storage method but is chosen for its simplicity and implementation with the 
    FEM. Future work may include a reduced row implementation but there will be
    a trade-off between memory minimization and computational efficiency.
    
    The \twotable method is composed of two separate matrices in memory. An 
    integer index table, \texttt{IDX}, and a double precision value table
    \texttt{VAL}. Each table is dimension $DOF \times D$ where $DOF$ is the
    number of degrees of freedom of the linear system and $D$ is the maximum
    number of nodes that a node shares including itself. This must be determined
    at the beginning of the problem.
    
    \texttt{IDX} is initialized to -1 and \texttt{VAL} is initialized 
    to 0.0 such that an index of -1 corresponds to a null entry in the 
    matrix. \texttt{IDX} is then populated with a modified adjacency graph. 
    Values in \texttt{IDX} indicate the column in which the \texttt{VAL} entry
    occurs. An example unstructured mesh is given in \fref{fig:adjacency_graph}. 
    Then, for node 5, the table \texttt{IDX} may resemble \eref{eq:idx_example}.
    \begin{equation}
      \label{eq:idx_example}
      \texttt{IDX}(5,:) = (8, 200, 48, 96, 5 )
    \end{equation}
    \begin{figure}
      \centering
      \includegraphics[width=0.5\textwidth]{adjacency_graph}
      \caption{Example Unstructured Mesh.}
      \label{fig:adjacency_graph}
    \end{figure}
    \eref{eq:idx_example} only an example because the order of the nodes in row
    5 is arbitrary. Similarly, a numeric example is provided in 
    \eref{eq:idx_number} (note that the value ``3'' is arbitrary).
    \begin{equation}
      \label{eq:idx_number}
      \left.
      \begin{array}{c}
        \texttt{IDX}(7,3) = 8 \\
        \texttt{VAL}(7,3) = 12.1
      \end{array}
      \right\}
      \implies
      A_{7,8} = 12.1
    \end{equation}
    \eref{eq:idx_number} indicates that the value of matrix $\ma$ in the
    seventh row in the eighth column is 12.1. This will allow for simple row 
    operations and efficient matrix vector multiplication which will be 
    necessary in the solution of the linear system.

  \subsection{Boundary Conditions}
    \label{sec:boundary_conditions}
    Boundary conditions deserve brief consideration. There are many ways to 
    implement boundary conditions and all result in mathematically the same 
    answer. The choices made for this application are presented below. Mirror 
    boundary conditions require $\grad \phi_g(\vr) = 0$ for 
    $\vr \in \partial \Omega$. These are also known as ``natural'' boundary
    conditions because the finite element matrix $\ma$ requires no additional 
    treatment and this condition is natural. In the albedo representation, this
    is equivalent to $\albedo = 0$.
    
    Albedo boundary conditions are treated with an additional contribution to 
    the finite element matrix $\ma$. These contributions represent a line 
    integral in two dimensions and a surface integral in three dimensions. These
    values are found in \eref{eq:matrix_population} and the quantities are 
    expressed in \sref{sec:matrix_quantities} or by quadratures 
    \sref{sec:quadratures}.
    
    Zero-flux boundary conditions require $\phi(\vr) = 0$ for 
    $\vr \in \partial \Omega$. These are treated by removing these entries from
    the finite element matrix $\ma$. Entries are removed by using an index 
    vector. In a natural system (see mirror boundary conditions above) each node
    corresponds to a row/column of the matrix. With entries removed, node number
    and index number may not exactly agree. A vector \texttt{ID} is introduced
    \cite{textbookjohnson}. Nodes with non-zero flux boundary conditions are set
    to a sequential positive integer. Nodes with zero-flux boundary conditions 
    are set to a negative integer (-1) and are omitted in the actual solution of 
    the system. Other strategies have been proposed such as the penalty approach 
    \cite{textbookhughes} and manually forcing the solution of the linear system 
    \cite{textbookli}. This method is chosen because it decreases the degree of 
    freedom of the linear system while encouraging a well conditioned matrix. 
    Now, the degree of freedom of the matrix is equal to the number of nodes 
    with non-zero flux boundary conditions. Alternatively, zero-flux boundary
    conditions can be represented as $\albedo \rightarrow \infty$.
    
  \subsection{Linear System Solution}
    \label{sec:linear_system_solution}
    For a non-singular linear system $\ma \vu = \vf$, where $\ma$ is a square 
    matrix, there exists a unique solution. Many strategies have been proposed 
    to solve this system in an efficient manner. Options are restricted in this
    application because the solution must operate with a sparsely stored linear 
    system and result in no fill-in. This immediately demands an iterative 
    method. The linear system described by the FEM can then be exploited for its 
    unique properties to select a favorable solution method.
    
    The finite element matrix $\ma$ for the problem described in 
    \eref{eq:multigroup_source} is Symmetric Positive Definite (SPD) if the
    multigroup equations are solved one group at a time. Note, the matrix will
    not have this especially useful property if all the groups are solved
    simultaneously. Symmetry condition is straight-forward and is observed in 
    the elemental matrix description \eref{eq:matrix_population}. Briefly,
    $A_{i,j,g,e}=A_{j,i,g,e}$.
    Positive definiteness is a particularly useful condition but is often 
    difficult to prove. $\ma$ is sometimes diagonally dominant for conveniently
    ordered meshes composed certain elements but generally, the matrix is not 
    diagonally dominant. 
    
    A matrix $\ma \in \realnn$ is positive definite if
    \begin{equation} \label{eq:positive_definite}
      \vx^{T} \ma \vx > 0 \qquad \forall \vx \in \realn, \; \vx \ne 0
    \end{equation}
    Following the proof of Theorem 1.9 in \cite{textbookhughes}. Let 
    $\vx = \{x_i\}$ for $i = 1,2,\ldots,N$. Then the vector-matrix and 
    matrix-vector products can be rewritten as summations.
    \begin{equation}
      \vx^{T} \ma \vx = \sum_{i=1}^{N} \sum_{j=1}^{N} x_i A_{ij} x_j
    \end{equation}
    By the definition of $A_{ij}$ in \eref{eq:matrix_population} and 
    \eref{eq:fem_notation}.
    \begin{equation}
      \vx^{T} \ma \vx = 
        \sum_{i=1}^{N} \sum_{j=1}^{N} x_i a(\basis_i,\basis_j) x_j
    \end{equation}
    Noting the property that $a(\cdot,\cdot)$ is a bilinear operator (i.e.
    linear in both arguments) \cite{textbookli}.
    \begin{equation}
      \vx^{T} \ma \vx =
        a \left( \sum_{i=1}^{N} x_i \basis_i, \sum_{j=1}^{N} x_j \basis_j 
        \right)
    \end{equation}
    By construction of the FEM, $w(\vr) = \sum_{i=1}^{N} x_i \basis_i$ where 
    $w(\vr)$ is a piecewise continuous polynomial of arbitrary order.
    \begin{equation}
      \vx^{T} \ma \vx = a \left(w(\vr),w(\vr)\right)
    \end{equation}
    $a(\cdot,\cdot)$ can be shown to form a norm $\|\cdot \|_a$
    \cite{textbookli} satisfying the positive definite condition
    \eref{eq:positive_definite}.
    \begin{equation}
      \vx^{T} \ma \vx > 0 \qquad \forall \vx \in \realn, \; \vx \ne 0
    \end{equation}
    
    A given matrix can be verified as positive definite by one of two methods.
    First, all eigenvalues of the matrix have positive real components. Second, 
    the matrix has a Cholesky decomposition such that $\ma = \ml \ml^*$ where 
    $\ml$ is a lower triangular matrix with positive diagonal entries and 
    $\ml^*$ is the conjugate transpose of the matrix \cite{textbookipsen}. For 
    real valued matrices, the conjugate transpose is equivalent to the 
    conventional transpose. Though this may be useful for debugging or numerical
    analysis purposes, these operations are computationally expensive. Instead,
    this is verified here for the general matrix and not tested on-the-fly.
    
    Conventional methods used to solve a linear system described by an SPD
    matrix include Gauss-Seidel iteration with Successive Over-Relaxation (SOR) 
    and the Conjugate Gradient (CG) Krylov subspace method. SOR needs
    \textit{a priori} knowledge of the optimized over-relaxation factor
    $\omega_{opt}$ for good performanc. In practice, this is performed 
    analytically with contrived solutions or in a modified guess-and-check 
    method. For this application, the CG method is chosen because it requires no 
    \textit{a priori} knowledge and produced a solution to the same tolerance in 
    a comparable wall-time without the need for guess-and-check iterations.
    
    A simple recipe for the CG method is presented in Algorithm 2.4.1
    \cite{Kelley1995IterativeEquations} and its implementation in this 
    application is replicated in \algorithmref{algorithm:CG}.
    
    \begin{algorithm}
      \caption{Conjugate Gradient Method.}
      \label{algorithm:CG}
      \begin{algorithmic}[1]
        \State $k = 0$
        \State $\vr = \vb - \ma \vx$
        \State $\rho_k = \|\vr\|_2^2$
        \State $k = k + 1$
        \While{$\sqrt{\rho_{k-1}} > \epsilon \|\vb\|_2$}
          \If{$k=1$}
            \State $\vp = \vr$
          \Else
            \State $\beta = \rho_{k-1} / \rho{k-2}$
            \State $\vp = \vr + \beta \vp$
          \EndIf
          \State $\vw = \ma \vp$
          \State $\beta = \rho_{k-1} / \vp^{T} \vw$
          \State $\vx = \vx + \beta \vp$
          \State $\vr = \vr - \beta \vw$
          \State $\rho_k = \| \vr \|_2^2$
          \State $k=k+1$
        \EndWhile
      \end{algorithmic}
    \end{algorithm}
    
    In \algorithmref{algorithm:CG}, $\epsilon$ is a tolerance set by the user 
    and the square of the two-norm is most efficiently replaced by the 
    inner-product as in \eref{eq:two_norm_inner}. 
    \begin{equation}
      \label{eq:two_norm_inner}
      \|\vr\|_2^2 = \vr^{T} \vr
    \end{equation}
    It is noted that this method requires minimal storage with only four vectors 
    required ($\vx, \vw, \vp,$ and $\vr$). Additionally, two scalar products 
    are required and a single matrix-vector product which proves to be the most 
    computationally expensive \cite{Kelley1995IterativeEquations}. As most of 
    the computational time of the diffusion solutions is spent in the linear 
    system solution, is is crucial that this process be efficient.
